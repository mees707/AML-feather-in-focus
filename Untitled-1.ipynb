{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.load(\"feather-in-focus/class_names.npy\", allow_pickle=True).item()\n",
    "df = pd.read_csv(\"feather-in-focus/train_images.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = [item.split('.', 1)[1] for item in list(labels.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_to_grayscale(image):\n",
    "    img1 = image.convert('L')\n",
    "    return np.expand_dims(img1, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_shape = (224,224)\n",
    "labels = df['label'].to_list()\n",
    "tmp = []\n",
    "dir = 'feather-in-focus/train_images/train_images/'\n",
    "output_dir = './resized_images_train/'\n",
    "\n",
    "image_filenames = sorted([filename for filename in os.listdir(dir) if filename.endswith(\".jpg\")], key=lambda x: int(x.split('.')[0]))\n",
    "\n",
    "# Resize and save images\n",
    "for filename, label in zip(image_filenames, labels):\n",
    "    image_path = os.path.join(dir, filename)\n",
    "    img = Image.open(image_path)\n",
    "    norm_image = normalize_to_grayscale(img)\n",
    "    resized_img = cv2.resize(norm_image, common_shape)\n",
    "\n",
    "    # Create class subdirectory if it doesn't exist\n",
    "    class_dir = os.path.join(output_dir, f'class_{label}')\n",
    "    os.makedirs(class_dir, exist_ok=True)\n",
    "\n",
    "    # Save resized image\n",
    "    output_path = os.path.join(class_dir, filename)\n",
    "    cv2.imwrite(output_path, resized_img)\n",
    "\n",
    "    img.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, n_classes=200):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels=32,\n",
    "                kernel_size=5,\n",
    "                stride=1,\n",
    "                padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=64,\n",
    "                kernel_size=5,\n",
    "                stride=1,\n",
    "                padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(64 * 50 * 50, n_classes)\n",
    "\n",
    "        self.softmax_fn = torch.nn.Softmax(dim=0)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.out(x)\n",
    "        return x[0], x[1]\n",
    "    \n",
    "    def fit(self, x, y, epochs, lr=0.001):\n",
    "        self.train()\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        loss = nn.CrossEntropyLoss()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            for curr_x, curr_y in zip(x, y):\n",
    "\n",
    "                logits = self.forward(curr_x)[0]\n",
    "\n",
    "                print(logits)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3219 images belonging to 200 classes.\n",
      "Found 707 images belonging to 200 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(validation_split=0.2)  # 20% for validation\n",
    "\n",
    "train_data = train_datagen.flow_from_directory(\n",
    "    output_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=64,\n",
    "    subset='training'  # Specify 'training' to get the training data\n",
    ")\n",
    "\n",
    "validation_data = train_datagen.flow_from_directory(\n",
    "    output_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=64,\n",
    "    subset='validation'  # Specify 'validation' to get the validation data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 224, 224, 3)\n",
      "(64, 200)\n"
     ]
    }
   ],
   "source": [
    "for img_batch,img_label in train_data:\n",
    "    print(img_batch.shape)\n",
    "    print(img_label.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "resizing_and_rescaling = tf.keras.Sequential([\n",
    "    tf.keras.layers.experimental.preprocessing.Resizing(224,224),\n",
    "    tf.keras.layers.experimental.preprocessing.Rescaling(1.0/255)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.experimental.preprocessing.RandomContrast(0.3),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal_and_vertical'),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomZoom(0.3),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomRotation(0.2)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE=224\n",
    "CHANNELS=3\n",
    "BATCH_SIZE=8\n",
    "EPOCHS=10\n",
    "\n",
    "input_shape=(BATCH_SIZE , IMAGE_SIZE, IMAGE_SIZE, CHANNELS)\n",
    "\n",
    "model= tf.keras.models.Sequential([\n",
    "  resizing_and_rescaling,\n",
    "  data_augmentation,  \n",
    "  # Convolution layer 1\n",
    "  tf.keras.layers.Conv2D(filters=120, kernel_size=(3,3), strides=(1,1),padding='same',activation='relu',input_shape=input_shape),\n",
    "  tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
    "  # Convolution layer 2\n",
    "  tf.keras.layers.Conv2D(filters=100, kernel_size=(3,3),strides=(1,1),padding='same',activation='relu'),\n",
    "  tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
    "  # Convolution layer 3\n",
    "  tf.keras.layers.Conv2D(filters=80, kernel_size=(3,3),strides=(1,1),padding='same',activation='relu'),\n",
    "  tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
    "  # Convolution layer 4\n",
    "  tf.keras.layers.Conv2D(filters=60, kernel_size=(3,3),strides=(1,1),padding='same',activation='relu'),\n",
    "  tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
    "\n",
    "  # Flatten Layers\n",
    "  tf.keras.layers.Flatten(),\n",
    "\n",
    "  # Dense layers\n",
    "  tf.keras.layers.Dense(units=1000,activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.4),\n",
    "  tf.keras.layers.Dense(units=1000,activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.3),\n",
    "  tf.keras.layers.Dense(units=1000,activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(units=len(class_labels) ,activation='softmax')\n",
    "\n",
    "])\n",
    "\n",
    "model.build(input_shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2\n",
      "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2\n",
      "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2\n",
      "51/51 [==============================] - 244s 5s/step - loss: 4.9747 - accuracy: 0.0183 - val_loss: 4.9924 - val_accuracy: 0.0156\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\",\n",
    "             loss=\"categorical_crossentropy\",\n",
    "             metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(train_data , batch_size=8 ,epochs=1,\n",
    "                verbose=1,\n",
    "                validation_data=validation_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
